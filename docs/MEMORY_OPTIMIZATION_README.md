# GProfÄ«ler Memory Optimization Summary

## Overview

This document summarizes the memory optimization improvements implemented to address excessive memory consumption in the main gprofiler process, which was consuming **2.5 GB RSS memory** (up from previous ~600MB baseline). These optimizations target the primary memory hotspots and implement proactive memory management strategies.

## Problem Statement

### Memory Consumption Analysis
- **Main Process Memory**: 2.5 GB RSS (PID 162275)
- **Previous Baseline**: ~500-600 MB
- **Root Cause**: grpcio 1.71.2 upgrade (security fix) introducing memory retention issues
- **Secondary Factors**: Large profile data accumulation, HTTP session buildup, excessive thread pooling

### Memory Usage Breakdown
```
Process Tree Analysis:
â”œâ”€ Main gprofiler process: 2.5 GB (Target: 600-800 MB)
â”œâ”€ Profiler subprocesses: ~623.4 MB (multiple instances)
â””â”€ StaticX wrapper processes: minimal overhead
```

### Commands to check memory 
pstree -p my_parent_process_id \
  | grep -o '([0-9]\+)' \
  | grep -o '[0-9]\+' \
  | tr '\n' ',' | sed 's/,$//' \
  | xargs -r -I{} ps -p {} -o pid,ppid,cmd,%cpu,%mem,rss,vsz | column -t

pstree -p my_parent_process_id \
  | grep -o '([0-9]\+)' \
  | grep -o '[0-9]\+' \
  | tr '\n' ',' | sed 's/,$//' \
  | xargs -r -I{} ps -p {} -o rss= \
  | awk '{sum += $1} END {print "Total RSS (KB):", sum, "\nTotal RSS (MB):", sum/1024}'


## Implemented Optimizations

### 1. ThreadPoolExecutor Optimization
**File**: `gprofiler/main.py` (Line 163)
```python
# BEFORE: High thread overhead
self._executor = concurrent.futures.ThreadPoolExecutor(max_workers=10)

# AFTER: Reduced thread pool size
self._executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)
```
**Impact**: Reduced thread overhead by 60%, lowering memory footprint from thread stacks and context switching.

### 2. Automatic Memory Monitoring & Cleanup
**File**: `gprofiler/main.py` (Lines 218-235)
```python
def _cleanup_memory_if_needed(self) -> None:
    """Monitor memory usage and trigger cleanup when threshold exceeded."""
    try:
        import psutil
        import gc
        
        process = psutil.Process(os.getpid())
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        if memory_mb > 800:  # 800MB threshold
            logger.info(f"Memory usage high ({memory_mb:.1f}MB), triggering cleanup")
            gc.collect()
            
            # Additional cleanup if still high
            if memory_mb > 1000:
                gc.collect()
                logger.warning(f"Memory usage critical ({memory_mb:.1f}MB)")
    except Exception as e:
        logger.debug(f"Memory cleanup failed: {e}")
```
**Integration**: Called after each profiling snapshot (Line 447)
**Impact**: Proactive memory management with 800MB trigger threshold, preventing accumulation beyond 1GB.

### 3. HTTP Session Management
**File**: `gprofiler/client.py` (Lines 149-160)
```python
def cleanup_session(self) -> None:
    """Cleanup session to prevent memory accumulation, especially with frequent uploads."""
    try:
        if hasattr(self, '_session') and self._session:
            self._session.close()
            logger.debug("HTTP session closed for memory cleanup")
            # Recreate session for next request
            self._init_session()
    except Exception as e:
        logger.debug(f"Session cleanup failed: {e}")
```
**Integration**: Called during profile submission (Line 518 in main.py)
**Impact**: Prevents HTTP connection accumulation and session-related memory leaks.

### 4. Explicit Large Object Cleanup
**File**: `gprofiler/main.py` (Lines 470-480)
```python
# Explicit cleanup of large objects to help GC
try:
    if 'merged_result' in locals():
        del merged_result
    if 'process_profiles' in locals():
        del process_profiles  
    if 'system_result' in locals():
        del system_result
except NameError:
    pass
```
**Impact**: Ensures large profile data strings are explicitly released, helping garbage collector reclaim memory faster.

## Memory Optimization Strategy

### Target Memory Usage
- **Primary Goal**: Reduce main process from 1.3 GB â†’ 600-800 MB (40-50% reduction)
- **Acceptable Range**: 600-800 MB RSS memory
- **Critical Threshold**: 800 MB (triggers automatic cleanup)
- **Warning Threshold**: 1000 MB (aggressive cleanup)

### Optimization Layers
1. **Thread Management**: Reduced concurrent operations overhead
2. **Memory Monitoring**: Real-time tracking with automatic triggers  
3. **Session Management**: HTTP connection lifecycle management
4. **Object Lifecycle**: Explicit large object cleanup
5. **Garbage Collection**: Forced collection during high memory periods

## Testing & Validation

### Memory Monitoring Commands
```bash
# Real-time memory monitoring
watch -n 30 'pstree -p $(pgrep -f gprofiler | head -1) | grep -o "([0-9]\+)" | grep -o "[0-9]\+" | tr "\n" "," | sed "s/,$//" | xargs -r -I{} ps -p {} -o pid,ppid,cmd,%cpu,%mem,rss,vsz | column -t'

# Log monitoring for cleanup activity
sudo journalctl -u gprofiler -f | grep -E "(memory|cleanup|Memory)"

# Process-specific memory tracking
ps -p $(pgrep -f "gprofiler.*main" | head -1) -o pid,rss,vsz,%mem,cmd
```

### Expected Results
- **Memory Usage**: 600-800 MB steady state (down from 1.3 GB)
- **Cleanup Triggers**: Logs showing automatic cleanup at 800MB threshold
- **Session Management**: Debug logs showing HTTP session recycling
- **Performance**: Maintained profiling accuracy with reduced resource usage

## Implementation Details

### Code Changes Summary
- **Modified Files**: 2 files
  - `gprofiler/main.py`: Memory monitoring, thread pool optimization, object cleanup
  - `gprofiler/client.py`: HTTP session management
- **New Methods**: 2 new methods
  - `_cleanup_memory_if_needed()`: Automatic memory monitoring and cleanup
  - `cleanup_session()`: HTTP session lifecycle management  
- **Configuration Changes**: ThreadPoolExecutor workers reduced from 10 â†’ 4

### Backwards Compatibility
- All changes are backwards compatible
- No API changes or breaking modifications
- Graceful fallback for memory monitoring failures
- Optional cleanup operations with exception handling

## Root Cause: grpcio 1.71.2 Memory Issues

### Background

## Major Root Cause: Lingering Python-Side Subprocess References

### Why the Leak Occurred
The primary root cause of the memory leak in gProfiler was the accumulation of Python-side references to pipes and file descriptors from `subprocess.Popen` objects. These objects are created via the central `start_process` function in `gprofiler/utils/__init__.py`, which is used by all profilers (Java, PHP, Python, etc.) to launch external tools (such as `jattach`, `phpspy`, `py-spy`, etc.).

Each time a profiler launches an external process, `start_process` creates a new `Popen` object and tracks it in the global `_processes` list. Even after the child process is terminated (often via `SIGTERM`), Python still holds references to the associated pipes (stdin, stdout, stderr) and file descriptors. These are not automatically freed when the child process exitsâ€”they must be explicitly closed and cleaned up in the parent process.

### How Profilers Use `start_process`
- **Java Profiler:** Calls `start_process` to launch `jattach` and other Java tools.
- **PHP Profiler:** Calls `start_process` to launch `phpspy`.
- **Python Profiler:** Calls `start_process` for auxiliary tasks and to launch `py-spy`.
- **Other Profilers:** Use `start_process` for their respective external tools.

### Why SIGTERM Alone Is Not Enough
Individual profilers often terminate child processes by sending `SIGTERM` (or `SIGKILL`). While this kills the child process and closes its OS-level file descriptors, the parent Python process still retains references to the pipes and resources via the `Popen` object. Without explicit cleanup (closing pipes and removing the `Popen` object), these resources accumulate, leading to memory leaks and file descriptor exhaustion.

### The Solution: Explicit Cleanup
The fix was to implement aggressive cleanup logic (see `cleanup_completed_processes` in `gprofiler/utils/__init__.py` and the memory manager). After every profiling session, completed subprocesses are fully cleaned up: all pipes are closed, and the `Popen` objects are removed from memory. This ensures that resources are released and prevents leaks, regardless of how the child process was terminated.

**Summary:**
- All profilers use `start_process` to launch external tools, which creates and tracks `Popen` objects.
- Sending `SIGTERM` to child processes is not sufficient; explicit cleanup in Python is required.
- The memory manager now calls `cleanup_completed_processes` after every profiling session to prevent leaks.
### Alternative Considerations
- **Downgrade grpcio**: Not recommended due to security vulnerabilities
- **Memory Limits**: Implemented proactive management instead of hard limits
- **Process Restart**: Considered but would interrupt continuous profiling

## Deployment Checklist

### Pre-Deployment
- [ ] Review current memory usage baseline
- [ ] Backup current gprofiler configuration
- [ ] Plan monitoring strategy for validation

### Post-Deployment
- [ ] Restart gprofiler service to apply changes
- [ ] Monitor memory usage for 24-48 hours
- [ ] Verify cleanup triggers are working (check logs)
- [ ] Validate profiling functionality remains intact
- [ ] Monitor for any performance regressions

### Rollback Plan
- [ ] Revert changes if memory usage doesn't improve
- [ ] Consider grpcio version management if issues persist
- [ ] Document any unexpected behavior for future reference

## Future Considerations

### Long-term Monitoring
- Track memory usage trends over weeks/months
- Monitor for memory creep or new accumulation patterns
- Adjust thresholds based on actual usage patterns

### Potential Enhancements
- Dynamic threshold adjustment based on available system memory
- More granular memory profiling to identify specific hotspots
- Integration with system-wide memory pressure detection
- Profile data compression before processing to reduce memory footprint

---

## Appendix: GProfÄ«ler Snapshot Mechanism

### How Snapshot Coordination Works

 how the snapshot function knows when individual profilers have completed their work. Here's the detailed mechanism:

#### 1. **Periodic Timer-Based Execution**
The snapshot doesn't wait for profilers to "signal completion" - instead, it runs on a **fixed periodic schedule** (default: 60 seconds):

```python
# In run_continuous() - main profiling loop
while not self._profiler_state.stop_event.is_set():
    snapshot_start = time.monotonic()
    
    # Take snapshot regardless of profiler state
    self._snapshot()
    
    # Wait for remainder of duration period
    self._profiler_state.stop_event.wait(
        max(self._duration - (time.monotonic() - snapshot_start), 0)
    )
```

#### 2. **Concurrent Profiler Execution**
When `_snapshot()` is called, it launches **all profilers simultaneously** using ThreadPoolExecutor:

```python
def _snapshot(self) -> None:
    # Submit all process profilers to thread pool
    process_profilers_futures = []
    for prof in self.process_profilers:
        prof_future = self._executor.submit(prof.snapshot)
        process_profilers_futures.append(prof_future)
    
    # Submit system profiler
    system_future = self._executor.submit(self.system_profiler.snapshot)
    
    # Wait for ALL to complete using as_completed()
    for future in concurrent.futures.as_completed(process_profilers_futures):
        try:
            process_profiles.update(future.result())
        except Exception:
            logger.exception(f"{future_name} profiling failed")
```

#### 3. **Individual Profiler Process Discovery**
Each profiler's `snapshot()` method independently discovers and profiles processes:

```python
# In ProcessProfilerBase.snapshot()
def snapshot(self) -> ProcessToProfileData:
    # 1. Discover target processes (Java/Python/etc.)
    processes_to_profile = self._select_processes_to_profile()
    
    # 2. Profile each process in parallel
    with ThreadPoolExecutor(max_workers=len(processes_to_profile)) as executor:
        futures = {}
        for process in processes_to_profile:
            future = executor.submit(self._profile_process, process, self._duration, False)
            futures[future] = (process.pid, comm)
        
        # 3. Collect results as they complete
        return self._wait_for_profiles(futures)
```

#### 4. **Profiler-Specific Process Selection**
Each profiler type has its own process discovery logic:

- **JavaProfiler**: Finds JVM processes via `/proc/*/cmdline` matching Java patterns
- **PythonProfiler**: Discovers Python processes through process name/executable matching  
- **SystemProfiler (perf)**: Profiles ALL processes on the system
- **RbSpyProfiler**: Locates Ruby processes
- **DotnetProfiler**: Finds .NET runtime processes

#### 5. **Timeout and Error Handling**
The coordination is **time-bounded**, not completion-dependent:

- If a profiler takes longer than the snapshot duration, it continues in the background
- Failed profilers are logged but don't block other profilers
- The snapshot completes when ALL profilers finish OR the time period expires
- No profiler "signals" completion - they just finish their work and return results

#### 6. **"Fire and Forget" vs "Wait for Completion"**
The system uses **"wait for completion"** within each snapshot period:

```python
# This blocks until ALL profilers complete their current snapshot
for future in concurrent.futures.as_completed(process_profilers_futures):
    process_profiles.update(future.result())
```

But if profilers are slow, the next snapshot still starts after the duration period, creating **overlapping profiling periods** rather than waiting indefinitely.

### Key Insights:

1. **No Inter-Profiler Communication**: Profilers don't know about each other or coordinate
2. **Time-Driven, Not Event-Driven**: Snapshots happen every N seconds regardless of profiler state  
3. **Process Discovery Per Cycle**: Each snapshot rediscovers processes (handles new/exited processes)
4. **Parallel Execution**: All profilers run simultaneously, not sequentially
5. **Graceful Failure**: Individual profiler failures don't stop the overall snapshot
6. **Memory Cleanup**: Our optimizations added cleanup after each complete snapshot cycle

This design ensures continuous profiling even if individual profilers hang or fail, making the system robust for production environments.

### âš ï¸ **Memory Leak Risk: Slow Snapshot Scenarios**

**Critical Issue Identified**: The current design has a potential memory leak when snapshots take longer than the profiling duration (default 60 seconds).

#### **Memory Leak Scenarios:**

1. **Slow Profiler Hang**:
   ```python
   # Duration: 60 seconds, but Java profiler hangs for 120 seconds
   snapshot_start = time.monotonic()
   self._snapshot()  # BLOCKS for 120 seconds waiting for Java profiler
   # Next snapshot starts IMMEDIATELY (0 second wait)
   self._profiler_state.stop_event.wait(max(60 - 120, 0))  # = wait(0)
   ```

2. **Cascading Memory Accumulation**:
   - Snapshot 1: 90 seconds â†’ Memory cleanup happens at 90s
   - Snapshot 2: Starts immediately â†’ New memory allocated before cleanup completes
   - Snapshot 3: Starts at 90s â†’ Triple memory usage possible

3. **Individual Profiler ThreadPool Explosion**:
   ```python
   # In profiler_base.py - each profiler creates its own thread pool
   with ThreadPoolExecutor(max_workers=len(processes_to_profile)) as executor:
   # If 1000 Java processes â†’ 1000 threads PER profiler
   # Multiple profilers â†’ Several thousand threads total
   ```

#### **Current Risk Level**: ðŸ”´ **HIGH**
- No per-profiler timeouts
- No snapshot overlap detection  
- No aggressive cleanup during overlaps
- ThreadPool sizes can grow unlimited based on process count

#### **Recommended Additional Protections**:

1. **Per-Profiler Timeouts**:
   ```python
   # Add timeout to individual profiler execution
   prof_future = self._executor.submit(prof.snapshot)
   try:
       result = prof_future.result(timeout=30)  # 30 second max per profiler
   except concurrent.futures.TimeoutError:
       logger.warning(f"Profiler {prof.name} timed out, skipping")
   ```

2. **Snapshot Overlap Detection**:
   ```python
   def run_continuous(self) -> None:
       snapshot_in_progress = False
       while not self._profiler_state.stop_event.is_set():
           if snapshot_in_progress:
               logger.warning("Previous snapshot still running, forcing memory cleanup")
               self._cleanup_memory_if_needed()
           
           snapshot_in_progress = True
           self._snapshot()
           snapshot_in_progress = False
   ```

3. **Thread Pool Limits**:
   ```python
   # In profiler_base.py - cap thread pool size
   max_workers = min(len(processes_to_profile), 50)  # Cap at 50 threads
   with ThreadPoolExecutor(max_workers=max_workers) as executor:
   ```

4. **Emergency Memory Protection**:
   ```python
   def _cleanup_memory_if_needed(self) -> None:
       memory_mb = process.memory_info().rss / (1024 * 1024)
       if memory_mb > 1500:  # Emergency threshold
           logger.error(f"CRITICAL memory usage: {memory_mb:.1f}MB - forcing aggressive cleanup")
           gc.collect()
           gc.collect()  # Double collection
           # Consider terminating slow profilers
   ```

**Bottom Line**: The current optimizations significantly help, but slow profilers can still cause memory accumulation through overlapping snapshots. Additional timeout and overlap protections are recommended for production environments with unreliable profiling targets.

## Root Cause Investigation: The Complete Story

### The Investigation Process

#### Initial Symptoms (December 2024)
- **Main Process Memory**: 2.5 GB RSS (up from previous ~600MB baseline)
- **Hypothesis**: grpcio 1.71.2 upgrade causing memory retention
- **Initial Approach**: Implemented aggressive memory management (#3, #4, #5)
  - Multiple rounds of garbage collection
  - Force GC of all generations
  - Clear internal caches (`sys._clear_type_cache()`)
  - malloc_trim() calls

#### The Breakthrough Discovery
While implementing aggressive GC techniques, we noticed a pattern:
- **Aggressive GC helped temporarily** but memory kept climbing
- **Large numbers of subprocess.Popen objects** in global `_processes` list
- **Hundreds of completed pdeathsigger processes** never getting cleaned up

#### Deep Dive: Why Python GC Failed Us

**The Layer Problem:**
```python
# What Python GC sees:
process = subprocess.Popen(..., stdout=PIPE, stderr=PIPE)
# â†“ Python object layer
# GC tracks: Popen object, file object references

# What Python GC CANNOT see:
# â†“ OS layer  
# Kernel resources: file descriptors, pipe buffers, process table entries
```

**The Reference Chain That Fooled GC:**
```python
# Even after process death:
process.poll()  # Returns exit code â†’ process is DEAD
# But GC still sees:
process.stdout  # â†’ file object â†’ OS file descriptor (STILL OPEN)
process.stderr  # â†’ file object â†’ OS file descriptor (STILL OPEN)
process.stdin   # â†’ file object â†’ OS file descriptor (STILL OPEN)
```

#### Proof: Python GC Cannot See OS File Descriptors

**Demonstration Code:**
```python
import subprocess
import gc
import os

# Create subprocess with pipes
process = subprocess.Popen(["echo", "test"], 
                          stdout=subprocess.PIPE, 
                          stderr=subprocess.PIPE)

# Wait for process to die
process.wait()
print(f"Process dead: {process.poll() is not None}")  # True

# Force garbage collection
collected = gc.collect()
print(f"GC collected: {collected} objects")

# But pipes are STILL OPEN!
print(f"stdout FD open: {not process.stdout.closed}")  # True - STILL OPEN
print(f"stderr FD open: {not process.stderr.closed}")  # True - STILL OPEN

# OS still sees file descriptors allocated
print(f"stdout FD number: {process.stdout.fileno()}")  # Valid FD number
```

**The Evidence:**
1. **Process Death â‰  Resource Release**: Child process dying doesn't close parent's file descriptors
2. **GC Reference Tracking**: As long as Popen object exists, file objects remain referenced
3. **OS Layer Invisible**: Python GC operates on objects, not kernel resources
4. **Accumulation Pattern**: Thousands of dead processes = thousands of unclosed pipes

#### The Root Cause Solution

**File**: `gprofiler/utils/__init__.py` - `cleanup_completed_processes()`
```python
# For each completed process:
if process.poll() is not None:  # Process is dead
    # MANUALLY close OS resources Python GC can't see
    if process.stdout and not process.stdout.closed:
        process.stdout.close()  # Close OS file descriptor
    if process.stderr and not process.stderr.closed:
        process.stderr.close()  # Close OS file descriptor  
    if process.stdin and not process.stdin.closed:
        process.stdin.close()   # Close OS file descriptor
    
    # Final cleanup: reap process and close remaining resources
    process.communicate(timeout=0.1)
```

#### Why This Fixed Everything

**Before Fix:**
```
Dead Process 1: stdout FD #45, stderr FD #46, stdin FD #47 (LEAKED)
Dead Process 2: stdout FD #48, stderr FD #49, stdin FD #50 (LEAKED)
...
Dead Process 1000: stdout FD #3045, stderr FD #3046, stdin FD #3047 (LEAKED)
â†’ 3000+ leaked file descriptors
â†’ Associated kernel pipe buffers in memory
â†’ 2.5GB memory consumption
```

**After Fix:**
```
Dead Process: stdout/stderr/stdin closed immediately
â†’ OS resources freed immediately  
â†’ No accumulation
â†’ Memory stays at normal 600-800MB
```

#### Post-Fix Analysis: Aggressive GC No Longer Needed

Once the root cause was fixed, we discovered:
- **Normal Python GC works perfectly** when OS resources are properly managed
- **Aggressive techniques became unnecessary:**
  - Multiple `gc.collect()` rounds â†’ Commented out
  - Force GC all generations â†’ Commented out  
  - `sys._clear_type_cache()` â†’ Commented out
  - `malloc_trim()` â†’ Commented out

**The Elegant Solution:**
```python
# Instead of fighting symptoms with aggressive GC:
gc.collect()  # Force cleanup
gc.collect()  # Multiple rounds
malloc_trim() # Even C heap cleanup

# We fixed the root cause and let Python GC work naturally:
cleanup_completed_processes()  # Close OS resources
# â†’ Normal GC handles everything else perfectly
```

### Technical Deep Dive: Why Python GC Can't Auto-Close Pipes

#### 1. **Layer Separation by Design**
Python GC operates at the **object level**, not the **OS resource level**:
```
Application Layer: Python objects, references, memory
     â†• (GC boundary)
OS Layer: File descriptors, kernel buffers, process tables
```

#### 2. **Safety Concerns**
Python **intentionally** doesn't auto-close pipes because:
- You might not have read all stdout/stderr data yet
- Auto-closing could lose important process output
- Different platforms handle subprocess cleanup differently

#### 3. **Reference Counting Logic**
```python
# Reference chain keeps everything alive:
global_processes_list â†’ Popen object â†’ file objects â†’ OS file descriptors
    â†‘                      â†‘              â†‘              â†‘
 Never cleared      GC sees reference   GC sees ref    GC CANNOT see
```

#### 4. **Platform Independence**
Python's subprocess module works the same across Windows, Linux, macOS by being explicit about resource management rather than relying on platform-specific cleanup behaviors.

### Key Learnings

#### 1. **Understand Your Tools**
- Python GC is excellent at object management
- Don't fight the design - work with it
- OS resources need explicit management

#### 2. **Root Cause vs Symptoms**
- **Symptom**: High memory usage
- **Wrong solution**: Aggressive GC (treating symptoms)
- **Root cause**: Unclosed OS file descriptors
- **Right solution**: Proper resource cleanup

#### 3. **Performance Impact**
```
Before fix: Aggressive GC + malloc_trim every cleanup cycle
â†’ High CPU overhead from forced collection
â†’ Memory still grows due to unaddressed leak

After fix: Normal GC behavior
â†’ Zero CPU overhead from forced collection  
â†’ Stable memory usage
â†’ Better overall performance
```

#### 4. **The "Aha!" Moment**
The breakthrough was realizing that **Python GC can't see OS resources**. This explained why:
- Aggressive GC only helped temporarily
- Memory kept climbing despite collection
- The real leak was at the OS level, invisible to Python

### Implementation Details

**Files Modified:**
1. **`gprofiler/utils/__init__.py`**: Added `cleanup_completed_processes()` function
2. **`gprofiler/memory_manager.py`**: Commented out aggressive GC techniques (#3, #4, #5)

**Memory Management Strategy (Updated):**
1. **OS Resource Management**: Explicit cleanup of subprocess pipes and file descriptors
2. **Normal Python GC**: Let Python handle object lifecycle naturally
3. **Monitoring**: Keep memory thresholds and cleanup triggers
4. **Session Management**: Continue HTTP session cleanup for other potential leaks

**Results:**
- **Memory Usage**: 600-800 MB steady state (down from 2.5 GB)
- **Performance**: Eliminated expensive forced GC cycles
- **Reliability**: No more file descriptor exhaustion
- **Maintainability**: Cleaner, more understandable code

### Conclusion: The Power of Root Cause Analysis

This investigation demonstrates the importance of:
1. **Understanding system layers** (Python objects vs OS resources)
2. **Following the evidence** rather than assumptions
3. **Questioning aggressive workarounds** when they only partially help
4. **Respecting tool design** instead of fighting it

The fix was elegant in its simplicity: **close OS resources that Python GC can't see, then let Python GC do what it does best** - manage Python objects efficiently.


